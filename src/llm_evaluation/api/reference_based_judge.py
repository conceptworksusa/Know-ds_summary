from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import PromptTemplate
from pydantic import BaseModel, Field
from typing import Optional
from fastapi import HTTPException
from KnowtionPOC.intent_classification.utilities.OllamaPipeline import OllamaPipeline
from KnowtionPOC.intent_classification.conf.Configurations import DEFAULT_LLAMA_MODEL , logger
from KnowtionPOC.intent_classification.conf.Prompts import HALLUCINATION_SYSTEM_PROMPT, CONTEXT_RELEVANCE_PROMPT, CORRECTNESS_PROMPT


class EvalHallucination(BaseModel):
    generated_text: str = Field(..., description="The text generated by the LLM.")
    context: str = Field(..., description="The context or reference text against which the generated text is evaluated.")
    answer: str = Field(..., description="Answer to the question which is either Faithful or Hallucinated.")

class EvalContextRelevance(BaseModel):
    question: str = Field(..., description="The question or prompt that was used to evaluate the context relevance.")
    context: str = Field(..., description="The context or reference text against which the question is evaluated.")
    answer: str = Field(..., description="Answer to the question which is either Relevant or Not Relevant.")

class EvalCorrectness(BaseModel):
    generated_text: str = Field(..., description="The text generated by the LLM.")
    reference_answer: str = Field(..., description="The reference answer against which the generated text is evaluated.")
    answer: str = Field(..., description="Answer to the question which is either Correct or Incorrect.")

class RagBasedJudge:


    def __init__(self, llama_model: Optional[str] = DEFAULT_LLAMA_MODEL):
         # Initialize the logger
         self.logger = logger

         try:
             # Initialize the OllamaPipeline model
             self.llm = OllamaPipeline(llama_model).get_model()
             self.logger.info("Ollama model initialized successfully")

         except Exception as e:
             self.logger.info(f"Error initializing the OllamaPipeline model: {str(e)}")
             raise HTTPException(status_code=500, detail="Error initializing the OllamaPipeline model")

    def context_relevance(self, question: str, context: str, system_prompt: Optional[str] = CONTEXT_RELEVANCE_PROMPT):
        """
        Evaluate the relevance of the context to the question.
        Args:
            question: The question or prompt that was used to evaluate the context relevance.
            context: The context or reference text against which the question is evaluated.
            system_prompt: The system prompt to guide the evaluation process.

        Returns: Returns an EvalContextRelevance object containing the evaluation result.

        """

        parser = PydanticOutputParser(pydantic_object=EvalContextRelevance)

        prompt = PromptTemplate(
            template=system_prompt + "\n {format_instructions}\n",
            input_variables=["question", "context"],
            partial_variables={"format_instructions": parser.get_format_instructions()}
        )

        try:
            self.logger.info("Evaluating context relevance...")
            response = self.llm.invoke(
                input=prompt.format(question=question, context=context)
            )
            self.logger.info("Response received from the model.")

            # Parse the response
            parsed_response = parser.invoke(response)
            return parsed_response

        except Exception as e:
            self.logger.error(f"Error evaluating context relevance: {e}")
            raise HTTPException(status_code=500, detail="Error evaluating context relevance")

    def evaluate_hallucination(self, generated_text:str, context:str, system_prompt:Optional[str] = HALLUCINATION_SYSTEM_PROMPT):
        """
        Evaluate the generated text for hallucination against the provided context.
        Args:
            generated_text: The text generated by the LLM that needs to be evaluated for hallucination.
            context: The context or reference text against which the generated text is evaluated.
            system_prompt: The system prompt to guide the evaluation process.

        Returns: Returns an EvalHallucination object containing the evaluation result.

        """

        parser = PydanticOutputParser(pydantic_object=EvalHallucination)

        prompt = PromptTemplate(
             template=system_prompt + "\n {format_instructions}\n",
             input_variables=["generated_text", "context"],
             partial_variables={"format_instructions": parser.get_format_instructions()}
         )

        try:
            self.logger.info("Evaluating hallucination...")
            response = self.llm.invoke(
                input=prompt.format(generated_text=generated_text, context=context)
            )
            self.logger.info("Response received from the model.")

            # Parse the response
            parsed_response = parser.invoke(response)
            return parsed_response

        except Exception as e:
            self.logger.error(f"Error evaluating hallucination: {e}")
            raise HTTPException(status_code=500, detail="Error evaluating hallucination")

    # Evaluating correctness based on reference answer.
    def evaluate_correctness(self, generated_text: str, reference_answer: str, system_prompt: Optional[str] = CORRECTNESS_PROMPT):
        """
        Evaluate the generated text for correctness against the provided reference answer.
        Args:
            generated_text: The text generated by the LLM that needs to be evaluated for correctness.
            reference_answer: The reference answer against which the generated text is evaluated.
            system_prompt: The system prompt to guide the evaluation process.

        Returns: Returns an EvalHallucination object containing the evaluation result.

        """
        parser = PydanticOutputParser(pydantic_object=EvalCorrectness)

        prompt = PromptTemplate(
            template=system_prompt + "\n {format_instructions}\n",
            input_variables=["generated_text", "reference_answer"],
            partial_variables={"format_instructions": parser.get_format_instructions()}
        )

        try:
            self.logger.info("Evaluating correctness...")
            response = self.llm.invoke(
                input=prompt.format(generated_text=generated_text, reference_answer=reference_answer)
            )
            self.logger.info("Response received from the model.")

            # Parse the response
            parsed_response = parser.invoke(response)
            return parsed_response

        except Exception as e:
            self.logger.error(f"Error evaluating correctness: {e}")
            raise HTTPException(status_code=500, detail="Error evaluating correctness")


if __name__ == "__main__":
    # Example usage of the RagBasedJudge class
    judge = RagBasedJudge()

    # Example evaluation for hallucination
    generated__text = "Paris is the capital of France"
    context1 = "The capital of France is Paris."

    try:
        result = judge.evaluate_hallucination(generated__text, context1)
        print(f"Evaluation Result: {result.answer}")
    except HTTPException as e1:
        print(f"Error: {e1}")

    generated__text = "Paris is the capital of United States"
    context1 = "The capital of France is Paris."
    try:
        result = judge.evaluate_hallucination(generated__text, context1)
        print(f"Evaluation Result: {result.answer}")
    except HTTPException as e1:
        print(f"Error: {e1}")


    # Example evaluation for context relevance
    query = "What is the capital of France?"
    context2 = "Elon Musk is the CEO of SpaceX and Tesla."
    try:
        relevance_result = judge.context_relevance(query, context2)
        print(f"Relevance Evaluation Result: {relevance_result.answer}")
    except HTTPException as e1:
        print(f"Error: {e1}")

    query = "Who maintains SpaceX?"
    context2 = "Elon Musk is the CEO of SpaceX and Tesla."
    try:
        relevance_result = judge.context_relevance(query, context2)
        print(f"Relevance Evaluation Result: {relevance_result.answer}")
    except HTTPException as e1:
        print(f"Error: {e1}")

    # Example evaluation for correctness
    generated_text1 = "The capital of France is Paris."
    reference_answer1 = "Paris is the capital of France."
    try:
        correctness_result = judge.evaluate_correctness(generated_text1, reference_answer1)
        print(f"Correctness Evaluation Result: {correctness_result.answer}")
    except HTTPException as e1:
        print(f"Error: {e1}")

    generated_text2 = "The capital of France is Berlin."
    reference_answer2 = "Paris is the capital of France."
    try:
        correctness_result = judge.evaluate_correctness(generated_text2, reference_answer2)
        print(f"Correctness Evaluation Result: {correctness_result.answer}")
    except HTTPException as e1:
        print(f"Error: {e1}")
