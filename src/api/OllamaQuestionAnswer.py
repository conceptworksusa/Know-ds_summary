# Importing necessary classes
from src.utilities.OllamaPipeline import OllamaPipeline
from src.conf.Configurations import logger
from fastapi import HTTPException
from src.conf.Prompts import llama3_prompt_for_qa
from src.conf.Configurations import DEFAULT_LLAMA_MODEL
from typing import Optional


class OllamaQuestionAnswer:
    def __init__(self, llama_model: Optional[str] = DEFAULT_LLAMA_MODEL):
        """
        Initialize the OllamaSummarizer class with the OllamaPipeline model and logger
        """

        # Initialize the logger
        self.logger = logger

        try:
            # Initialize the OllamaPipeline model
            self.llm = OllamaPipeline(llama_model).get_model()
            self.logger.info("Ollama model initialized successfully")

        except Exception as e:
            self.logger.info(f"Error initializing the OllamaPipeline model: {str(e)}")
            raise HTTPException(status_code=500, detail="Error initializing the OllamaPipeline model")

    def qa_with_ollama(self, context: str, questions: list[str]):
        """
        Perform question answering using the Ollama model
        :param context: The context in which to answer the question.
        :param questions: The questions to answer.
        :return: The answer generated by the Ollama model.
        """

        # Invoke the model with the chat structure
        try:
            # Directly invoke the model with the formatted prompt
            self.logger.info("invoking the model with input message")
            response = self.llm.invoke(input=llama3_prompt_for_qa.format(questions=questions, context=context))
            self.logger.info("response received from the model")
            self.logger.info(f"response: {response}")

            return response
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"An error occurred during invocation: {e}")


if __name__ == "__main__":

    sample_context = 'The following discussion is mostly presented in terms of linear functions but the use of least squares is valid and practical for more general families of functions. Also, by iteratively applying local quadratic approximation to the likelihood (through the Fisher information), the least-squares method may be used to fit '

    QUESTIONS = ["how to apply quadratic function?"]

    res = OllamaQuestionAnswer().qa_with_ollama(sample_context, QUESTIONS)

    print(res)
